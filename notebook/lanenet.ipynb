{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import pdb\n",
    "\n",
    "# supporting:\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from config import global_config\n",
    "from dataset import LaneNetDataset\n",
    "from model import vgg_encoder\n",
    "from model import fcn_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly loss calculation and inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encoder = vgg_encoder.VGGEncoder()\n",
    "        self.decoder = fcn_decoder.FCNDecoder()\n",
    "        self.conv1 = nn.Conv2d(64, 3, kernel_size=1, bias=False)  # pixembedding\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "\n",
    "    def inference(self, src):\n",
    "        decode_logits, decode_deconv  = self.run_model(src)\n",
    "        \n",
    "        binary_seg_ret = F.softmax(decode_logits)\n",
    "        binary_seg_ret = np.argmax(binary_seg_ret, dim=1)\n",
    "        \n",
    "        pix_embedding = F.relu(self.conv1(decode_deconv))\n",
    "        return (binary_seg_ret, pix_embedding)\n",
    "    \n",
    "    def run_model(self, src):\n",
    "        src_tensor = self.preprocess(src)\n",
    "\n",
    "        # have to check if batch or not\n",
    "        if len(src_tensor) != 4:\n",
    "            src_tensor = src_tensor.unsqueeze(0)\n",
    "        \n",
    "        # encode\n",
    "        ret = self.encoder(src_tensor)\n",
    "        # decode\n",
    "        decode_logits, decode_deconv  = self.decoder(ret)\n",
    "        return (decode_logits, decode_deconv)\n",
    "\n",
    "    def compute_loss(self, src, binary, instance):\n",
    "\n",
    "        decode_logits, decode_deconv  = self.run_model(src)\n",
    "\n",
    "        # step 1:\n",
    "        # calculate loss between binary and decode logits\n",
    "        #\n",
    "        # use softmax_cross_entropy\n",
    "        binary_segmenatation_loss = torch.sum(- binary * F.log_softmax(decode_logits, -1), -1)\n",
    "        binary_segmenatation_loss = binary_segmenatation_loss.mean()\n",
    "\n",
    "        # step 2:\n",
    "        # calculate discrimitive loss between deconv and instance\n",
    "        # change deconv into pix_embedding\n",
    "\n",
    "        # then calculate discrimitive loss\n",
    "        pix_embedding = F.relu(self.conv1(decode_deconv))\n",
    "        disc_loss, l_var, l_dist, l_reg = \\\n",
    "                lanenet_discriminative_loss.discriminative_loss(\n",
    "                    pix_embedding, instance, 3, 0.5, 1.5, 1.0, 1.0, 0.001)\n",
    "        \n",
    "        total_loss = 0.7*binary_segmentation_loss + 0.3*disc_loss\n",
    "        \n",
    "        ret = {\n",
    "            'total_loss': total_loss,\n",
    "            'binary_seg_logits': decode_logits,\n",
    "            'instance_seg_logits': pix_embedding,\n",
    "            'binary_seg_loss': binary_segmenatation_loss,\n",
    "            'discriminative_loss': disc_loss\n",
    "        }\n",
    "        \n",
    "        return ret\n",
    "        \n",
    "    def discrimitive_loss(self, prediction, correct_label, feature_dim,\n",
    "                        delta_v, delta_d, param_var, param_dist, param_reg):\n",
    "        \n",
    "        # saving list (maybe implement dynamic tensor?)\n",
    "        output_ta_loss = []\n",
    "        output_ta_var = []\n",
    "        output_ta_dist = []\n",
    "        output_ta_reg = []\n",
    "        \n",
    "        # for each batch calculate the loss\n",
    "        i = 0\n",
    "        while i < prediction.shape[0]:\n",
    "            # calculate discrimitive loss for single image\n",
    "            single_prediction = prediction[i]\n",
    "            single_label = correct_lable[i]\n",
    "            # pdb.set_trace()\n",
    "            disc_loss, l_var, l_dist, l_reg = single_discrimitive_loss(\n",
    "                single_prediction, single_label, feature_dim, delta_v, delta_d, param_var, param_dist, param_reg)\n",
    "            \n",
    "            output_ta_loss.append(disc_loss.unsqueeze(0))\n",
    "            output_ta_va.append(l_var.unsqueeze(0))\n",
    "            output_ta_dist.append(l_dist.unsqueeze(0))\n",
    "            output_ta_reg.append(l_reg.unsqueeze(0))\n",
    "            \n",
    "            i += 1  # next image in batch\n",
    "        \n",
    "        out_loss_op = torch.cat(output_ta_loss)\n",
    "        out_var_op = torch.cat(output_ta_var)\n",
    "        out_dist_op = torch.cat(output_ta_dist)\n",
    "        out_reg_op = torch.cat(output_ta_reg)\n",
    "        \n",
    "        # calculate mean of the batch\n",
    "        disc_loss = out_loss_op.mean()\n",
    "        l_var = out_var_op.mean()\n",
    "        l_dist = out_vdist_op.mean()\n",
    "        l_reg = out_reg_op.mean()\n",
    "\n",
    "        return disc_loss, l_var, l_dist, l_reg\n",
    "        \n",
    "    def discriminative_loss_single(\n",
    "            prediction,\n",
    "            correct_label,\n",
    "            feature_dim,\n",
    "            delta_v,\n",
    "            delta_d,\n",
    "            param_var,\n",
    "            param_dist,\n",
    "            param_reg):\n",
    "        \"\"\"\n",
    "        The example partition loss function mentioned in the paper equ(1)\n",
    "        :param prediction: inference of network\n",
    "        :param correct_label: instance label\n",
    "        :param feature_dim: feature dimension of prediction\n",
    "        :param delta_v: cutoff variance distance\n",
    "        :param delta_d: curoff cluster distance\n",
    "        :param param_var: weight for intra cluster variance\n",
    "        :param param_dist: weight for inter cluster distances\n",
    "        :param param_reg: weight regularization\n",
    "        \"\"\"\n",
    "\n",
    "        # Make it a single line\n",
    "        correct_label = correct_label.view([correct_label.shape[0] * correct_label.shape[1]]).float()\n",
    "        reshaped_pred = prediction.view([feature_dim, prediction[0] * prediction[1]]).float()\n",
    "        \n",
    "        # Get unique labels\n",
    "        unique_labels, unique_id = torch.unique(correct_label, sorted=True, return_inverse=True)\n",
    "        ids, counts = np.unique(unique_id, return_counts=True)\n",
    "        num_instances = len(counts)\n",
    "        counts = torch.tensor(counts, dtype=torch.float32)\n",
    "        \n",
    "        # Calculate the pixel embedding mean vector\n",
    "        segmented_sum = torch.zeros(feature_dim, num_instances).scatter_add(1, unique_id.repeat([feature_dim,1]), reshaped_pred)\n",
    "        mu = torch.div(segmented_sum, counts)\n",
    "        mu_expand = torch.gather(mu, 1, unique_id.repeat([feature_dim,1]))\n",
    "\n",
    "        # Calculate loss(var)\n",
    "        distance = (mu_expand - reshaped_pred).t().norm(dim=1)\n",
    "        distance -= torch.tensor(delta_v, dtype=torch.float32)\n",
    "        distance = torch.clamp(distance, min=0.)   # min is 0.\n",
    "        distance = distance.pow(2)\n",
    "        \n",
    "        l_var = torch.zeros(num_instances).scatter_add(0, unique_id, distance)\n",
    "        l_var = torch.div(l_var, counts)\n",
    "        l_var = l_var.sum()\n",
    "        l_var = torch.div(l_var, num_instances)  # single value \n",
    "   \n",
    "        # Calculate the loss(dist) of the formula\n",
    "        for i in range(feature_dim):\n",
    "            for j in range(feature_dim):\n",
    "                if i != j:\n",
    "                    diff = mu[i] - mu[j]\n",
    "                    mu_diff.append(diff.unsqueeze(0))\n",
    "                    \n",
    "        mu_diff = torch.cat(mu_diff)\n",
    "        \n",
    "        mu_norm = mu_diff.norm(dim=1)\n",
    "        mu_norm = (2. * delta_d - mu_norm)\n",
    "        mu_norm = torch.clamp(mu_norm, min=0.)\n",
    "        mu_norm = mu_norm.pow(2)\n",
    "        \n",
    "        l_dist = mu_norm.mean()\n",
    "        \n",
    "        # Calculate the regular term loss mentioned in the original Discriminative Loss paper\n",
    "        l_reg = mu.norm(dim=1).mean()\n",
    "\n",
    "        # Consolidation losses are combined according to the parameters mentioned in the original Discriminative Loss paper\n",
    "        param_scale = 1.\n",
    "        l_var = param_var * l_var\n",
    "        l_dist = param_dist * l_dist\n",
    "        l_reg = param_reg * l_reg\n",
    "\n",
    "        loss = param_scale * (l_var + l_dist + l_reg)\n",
    "\n",
    "        return loss, l_var, l_dist, l_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking 3/4d matrix and \n",
    "v = torch.randn(3, 4, 5)\n",
    "v_ = v.unsqueeze(0)\n",
    "# v_.shape\n",
    "\n",
    "l = torch.randn(4, 3, 5, 5)\n",
    "l.shape[0]\n",
    "# l[2].shape\n",
    "\n",
    "var = []\n",
    "for i in range(l.shape[0]):\n",
    "    l_ = l[0].unsqueeze(0)\n",
    "    var.append(l_)\n",
    "    \n",
    "la = torch.cat(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 5, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.7211, -2.7096,  0.3037,  0.2904, -1.1449],\n",
      "         [-1.9282, -1.0326, -1.1341,  0.1440,  0.7094],\n",
      "         [ 2.1354,  1.4281, -0.4329, -0.7835, -0.7612],\n",
      "         [ 1.3064, -0.4989, -0.0346,  1.5963,  1.4391],\n",
      "         [ 0.1881,  0.0033,  0.5158, -0.1344, -2.5246]],\n",
      "\n",
      "        [[-0.0331,  1.0059, -0.5271, -0.0486, -0.4435],\n",
      "         [ 0.7803,  0.2696, -1.0937, -0.2011, -1.2671],\n",
      "         [-0.0911,  0.1432, -0.7650, -0.6285,  1.1247],\n",
      "         [ 0.5227,  0.2437, -0.4807,  1.6500,  0.6388],\n",
      "         [ 1.3636, -0.7209, -0.5638,  1.5149,  0.0320]],\n",
      "\n",
      "        [[-0.2427, -0.1569, -0.4877,  1.5384, -0.1461],\n",
      "         [ 0.0479,  1.1628,  1.0367,  0.8321,  0.9458],\n",
      "         [-0.2865,  0.7631,  0.6680,  1.0855, -0.9787],\n",
      "         [-0.2787, -0.1016, -0.3290, -1.0247,  0.4326],\n",
      "         [ 0.2514,  0.8999, -0.0997,  1.3971,  0.2803]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7211, -2.7096,  0.3037,  0.2904, -1.1449, -1.9282, -1.0326, -1.1341,\n",
       "         0.1440,  0.7094,  2.1354,  1.4281, -0.4329, -0.7835, -0.7612,  1.3064,\n",
       "        -0.4989, -0.0346,  1.5963,  1.4391,  0.1881,  0.0033,  0.5158, -0.1344,\n",
       "        -2.5246, -0.0331,  1.0059, -0.5271, -0.0486, -0.4435,  0.7803,  0.2696,\n",
       "        -1.0937, -0.2011, -1.2671, -0.0911,  0.1432, -0.7650, -0.6285,  1.1247,\n",
       "         0.5227,  0.2437, -0.4807,  1.6500,  0.6388,  1.3636, -0.7209, -0.5638,\n",
       "         1.5149,  0.0320, -0.2427, -0.1569, -0.4877,  1.5384, -0.1461,  0.0479,\n",
       "         1.1628,  1.0367,  0.8321,  0.9458, -0.2865,  0.7631,  0.6680,  1.0855,\n",
       "        -0.9787, -0.2787, -0.1016, -0.3290, -1.0247,  0.4326,  0.2514,  0.8999,\n",
       "        -0.0997,  1.3971,  0.2803])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(la[0])\n",
    "la[0].view([3*5*5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, j = torch.unique(l[0], sorted=True, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([10, 5, 3, 0, 4, 5, 4, 5, 5, 3, 2, 1])\n",
    "a_ = torch.tensor([[9, 4, 3, 0, 3, 5, 3, 5, 6, 3, 2, 1], [8, 3, 3, 0, 3, 5, 3, 5, 7, 3, 1, 1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, unique_id = torch.unique(a, sorted=True, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 5, 3, 0, 4, 5, 4, 5, 5, 3, 2, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(unique_id, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6] [1 1 1 2 2 4 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels, counts)\n",
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9., 13.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segmented sum:\n",
    "\n",
    "index = torch.tensor([[0, 0, 1, 1, 0, 1]])\n",
    "data = torch.tensor([[5., 1., 7., 2., 3., 4.]])\n",
    "\n",
    "torch.zeros(1, 2).scatter_add(1, index, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([2, 12])\n",
      "7\n",
      "tensor([[6, 5, 3, 0, 4, 5, 4, 5, 5, 3, 2, 1],\n",
      "        [6, 5, 3, 0, 4, 5, 4, 5, 5, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(unique_id.shape)\n",
    "print(a_.shape)\n",
    "print(len(counts))\n",
    "print(unique_id.repeat([2,1]))\n",
    "segmented_sum = torch.zeros(2, len(counts)).scatter_add(1, unique_id.repeat([2,1]), a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg sum tensor([[ 0.,  1.,  2.,  6.,  6., 20.,  9.],\n",
      "        [ 0.,  1.,  1.,  6.,  6., 20.,  8.]])\n",
      "counts  tensor([1., 1., 1., 2., 2., 4., 1.])\n",
      "normalized  tensor([[0., 1., 2., 3., 3., 5., 9.],\n",
      "        [0., 1., 1., 3., 3., 5., 8.]])\n"
     ]
    }
   ],
   "source": [
    "a_count = torch.tensor(counts, dtype=torch.float32)\n",
    "# a_count = a_count.view(-1, 1)\n",
    "print('seg sum', segmented_sum)\n",
    "print('counts ', a_count)\n",
    "mu = torch.div(segmented_sum, a_count)\n",
    "print('normalized ', mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids  tensor([6, 5, 3, 0, 4, 5, 4, 5, 5, 3, 2, 1])\n",
      "mu  tensor([[0., 1., 2., 3., 3., 5., 9.],\n",
      "        [0., 1., 1., 3., 3., 5., 8.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5., 3., 0., 3., 5., 3., 5., 5., 3., 2., 1.],\n",
       "        [8., 5., 3., 0., 3., 5., 3., 5., 5., 3., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ids ', unique_id)\n",
    "print('mu ', mu)\n",
    "mu_expand = torch.gather(mu, 1, unique_id.repeat([2,1]))\n",
    "mu_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  0.,  0.,  0.,  0.,  0.,  0., -2.,  0.,  0.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 2.2361, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2361,\n",
       "        0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mu_expand-a_)\n",
    "distance = (mu_expand - a_).norm(dim=0)\n",
    "# distance = torch.norm(mu_expand - a_, 2, -1)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0300,  2.2061, -0.0300, -0.0300, -0.0300, -0.0300, -0.0300, -0.0300,\n",
       "         2.2061, -0.0300, -0.0300, -0.0300])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_v = 0.03\n",
    "distance -= torch.tensor(delta_v, dtype=torch.float32)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 2.2061, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2061,\n",
       "        0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = torch.clamp(distance, min=0.)\n",
    "distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 4.8667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.8667,\n",
       "        0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = distance.pow(2)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 9.7335, 0.0000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_var = torch.zeros(len(counts)).scatter_add(0, unique_id, distance)\n",
    "l_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4334, 0.0000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_var = torch.div(l_var, torch.tensor(counts, dtype=torch.float32))\n",
    "l_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3476)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_var = l_var.sum()\n",
    "l_var = torch.div(l_var, len(counts))\n",
    "l_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0., -1.,  0.,  0.,  0., -1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim = 2\n",
    "num_instances = len(counts)\n",
    "mu_diff = []\n",
    "\n",
    "# think of a better way to do this:\n",
    "for i in range(feature_dim):\n",
    "    for j in range(feature_dim):\n",
    "        if i != j:\n",
    "            diff = mu[i] - mu[j]\n",
    "            mu_diff.append(diff.unsqueeze(0))\n",
    "            \n",
    "mu_diff = torch.cat(mu_diff)\n",
    "mu_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4142, 1.4142])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_norm = mu_diff.norm(dim=1)\n",
    "mu_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5858, 4.5858])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_d = 3\n",
    "mu_norm = (2. * delta_d - mu_norm)\n",
    "mu_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.0294, 21.0294])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_norm = torch.clamp(mu_norm, min=0.)\n",
    "mu_norm = mu_norm.pow(2)\n",
    "mu_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.0294)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_dist = mu_norm.mean()\n",
    "l_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.3578, 10.4403])\n",
      "tensor(10.8991)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.8991)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mu.norm(dim=1))\n",
    "print(mu.norm(dim=1).mean())\n",
    "\n",
    "l_reg = mu.norm(dim=1).mean()\n",
    "l_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
